# Тестовое задание

* Реализовать сервис, который обходит произвольный сайт с глубиной до 2 и сохраняет `html`, `url` и `title` страницы в произвольное хранилище (на ваш выбор).

    Примеры сайтов:
    * `https://ria.ru`
    * `http://www.vesti.ru`
    * `http://echo.msk.ru`
    * `http://tass.ru/ural` 
    * `https://lenta.ru`
    
* Оптимизировать прогрузку по потреблению памяти и по времени. 
   Замерить время выполнения и потребление памяти.

* CLI.
    * По урлу сайта и глубине обхода загружаются данные.
    * По урлу сайта из хранилища можно получить `n` прогруженных страниц (`url` и `title`).
    
        Пример:
        ```
        spider.py load http://www.vesti.ru/ --depth 2
        >> ok, execution time: 10s, peak memory usage: 100 Mb
        spider.py get http://www.vesti.ru/ -n 2
        >> http://www.vesti.ru/news/: "Вести.Ru: новости, видео и фото дня"
        >> http://www.vestifinance.ru/: "Вести Экономика: Главные события российской и мировой экономики, деловые новости,  фондовый рынок"
        ```

* Требования
    * Язык реализации `python3`
    * Докеризовать сервис
    * Стек технологий произвольный
    * Решение оформить как проект на `github`
    * Описать в `README.md` установку и запуск